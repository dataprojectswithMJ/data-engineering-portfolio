{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911c7bf2-2660-4681-a40a-e640189d4a0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# South African COVID-19 Daily Hospital data API\n",
    "### Data engineering project\n",
    "#### Extraction phase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b56d9-10c7-43b0-90a0-fc46ef587a31",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63923c1-ffa6-4452-a4dd-58bc1431b3c4",
   "metadata": {},
   "source": [
    "The API and documentation is available on <a href='https://covidza-data.deta.dev/docs'>COVIDZA DATA API</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7830e5ac-567f-49e6-b09e-153ee9033225",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774527c-b629-4c8f-a766-324b876b4ad2",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0823f97-10cd-42de-bb3c-46620a0ff0e8",
   "metadata": {},
   "source": [
    "Looking for a data engineer, contact me:\n",
    " - [Linkedin]('https://linkedin.com/in/mpho-jan-kubeka')\n",
    " - [Github]('https://github.com/dataprojectswithMJ')\n",
    " - [Youtube]('https://www.youtube.com/channel/UClOP0fAisga04q3OB1iC4nQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94747e8-45d7-493a-9a81-37af44cd6848",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb690640-cab6-4ff9-9761-bcee078293c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## About the project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106d150-4764-46e7-bd6d-66301f53ce09",
   "metadata": {},
   "source": [
    "I was looking for some COVID stats in South Africa to build a dashboard and maybe run a time series forecasting model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f713576-7db3-4577-a185-d81682ceb649",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem with Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07883e1e-5096-4d4e-8fa5-b2f7bc48c679",
   "metadata": {},
   "source": [
    "There were a few problems with the extraction stage. The biggest issues:\n",
    "   - Reliable data source\n",
    "   - Usable format\n",
    "   - Open source API\n",
    "   \n",
    "But then I came across the <a href=\"https://www.nicd.ac.za/\">NICD website</a>. This site contains the data that I need however they have it in PDFs which is not optimum for my use case.\n",
    "\n",
    "Screenshot of the data in the PDF from the NICD website.\n",
    "\n",
    "<img src='PDF_sample.png' width='500'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f86c85f-20f2-46da-80b2-d693b845ccb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33673968-461e-423d-b26c-5b2ef9f70014",
   "metadata": {},
   "source": [
    "I then decided to build the API myself and make it open source. \n",
    "\n",
    "The biggest points of focus right now is automating the PDF downloads using __requests__ while storing the download links in a database (just for reference and backup).\n",
    "\n",
    "This is what the end result looks like:\n",
    "\n",
    "<img src='db_links_record.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44470e23-ac72-4b75-ae99-85763656648e",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd5d51-351e-47e0-bc9e-99abc56e3401",
   "metadata": {},
   "source": [
    "## Tech Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299950f-b6c1-40bb-bb6e-8b1ca8c6fff2",
   "metadata": {},
   "source": [
    "My tech stack for this stage includes:\n",
    "   - Python\n",
    "        - BeautifulSoup\n",
    "        - Requests\n",
    "        - PyMONGO (MongoDB Python Wrapper)\n",
    "        <br></br>\n",
    "   - MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d05545-b00a-415a-b31b-7a3c37a8f504",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ad798-d046-434b-9f7a-1fb8fa1c71b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c36c805-7bb8-4bc3-9e9c-25c2b210f63b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Importing the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201042db-1101-4bab-8910-ed1e2251211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c4b31-023f-48a4-855b-88399ae6c2ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Configure MongoDB instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e67ac0-e260-48ca-bed9-82afb6c08330",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = ' .... ' #uri is used to connect this wrapper to the database. Localhost for testing. Use ATLAS for cloud instance of MongoDB\n",
    "\n",
    "client = MongoClient(uri)\n",
    "\n",
    "db = client['COVIDAPI'] # Name of database where all the data will go into\n",
    "\n",
    "links = db['links'] #Name of the collection (not table because this is a NoSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc18f79-11ab-4fba-a70f-0f94a1d87569",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure the scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeb78ac4-74a2-49dd-a7e2-633cf203e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the NICD website where all the documents can be downloaded.\n",
    "\n",
    "url = 'https://www.nicd.ac.za/diseases-a-z-index/disease-index-covid-19/surveillance-reports/daily-hospital-surveillance-datcov-report/'\n",
    "\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc88788-20f1-4c48-856e-8b51ae016905",
   "metadata": {},
   "source": [
    "The 'soup' parameter should return a 200 status code. If the code is different then there has to be something wrong. Examples:\n",
    "    -400 range is a user error\n",
    "    -500 range is a server error could be related to the code or even the micro server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f575a2-9fe3-49b1-aa17-0d7549fb052b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now the scraping begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fdce2-90c3-4954-86d8-c326d59010a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return all PDF download links found\n",
    "\n",
    "result = soup.find_all(name='a', attrs={'class': 'elementor-button-link elementor-button elementor-size-xs'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee8f0e-e2bc-41e4-9c02-3f7c7cc65e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in result:\n",
    "    file_path = \" .... \"\n",
    "    \n",
    "    file_name = (re.findall(r'\\(.*?\\)', x.text))[0].replace(\"['(\", \"\").replace(\")']\", \"\").replace(\"\\xa0\", \"\").replace(\n",
    "        \"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "    if not links.find_one({'link': x['href']}):\n",
    "        links.insert_one({'title': x.text, 'link': x['href'], 'scrape_date': datetime.now()})\n",
    "        print('NEW FILE ADDED TO DB!!!')\n",
    "\n",
    "        r = requests.get(x['href'], stream=True)\n",
    "\n",
    "        with open(f\"{file_path}{file_name}.pdf\", \"wb\") as pdf:\n",
    "            try:\n",
    "                for chunk in r.iter_content(chunk_size=1024):\n",
    "\n",
    "                    # writing one chunk at a time to pdf file\n",
    "                    if chunk:\n",
    "                        pdf.write(chunk)\n",
    "                print(f'Done with {file_name}.pdf')\n",
    "\n",
    "            except EOFError as e:\n",
    "                print(e)\n",
    "    else:\n",
    "        print('FILE ALREADY EXISTS!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de59264-2104-4ee3-96a4-8711fc6ebbf2",
   "metadata": {},
   "source": [
    "There is quite a bit happening in the above cell. We'll go through it step by step. Firstly, I want to define the variable file_name. \n",
    "  - __file_path__:\n",
    "       We are going to save/download the PDF documents to storage. This variable will tell the code where to save the files.\n",
    "  - __file_name__:\n",
    "       The web scraping returns the information with the specified name. We do not need all the extra 'things' such as brackets, text etc. This varible removes the extra information and leaves only the date.\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6713e9d-82f6-4830-8474-e9e0453304c5",
   "metadata": {},
   "source": [
    "I begin by telling the code to search in the database if the file name exists or not. If it does exist:\n",
    " - If it does __NOT__ exist:\n",
    "     1) The code will then store the following data into the database:\n",
    "        - the name of the file\n",
    "        - the link of the PDF\n",
    "        - the date the PDF was discovered\n",
    "     2) Use the link found to download the PDF using the __'open'__ function and write to the PDF file using iterating chunks of 1024 bits.\n",
    "     3) Then I save the PDF documents locally (at the moment) and use the __file_name__ variable as the name of the document.\n",
    "\n",
    "\n",
    "- If it does exist:\n",
    "     1) Prints 'FILE ALREADY EXISTS' and goes to the next link\n",
    "     \n",
    "### NB! This is part of the project's *LOADING* step of our ETL process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1671062c-c554-407e-a47d-206ff33a0a05",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762be81-2828-48a7-909e-4ed44b903742",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a8bd9b-ceec-490b-80dd-c96acc35fd77",
   "metadata": {},
   "source": [
    "#### This is the end of the scraping and downloading the PDF documents to storage. The *transformation* code is another notebook in this repo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
