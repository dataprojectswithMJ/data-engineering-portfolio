{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4150f00-8f11-4ca0-bf7e-e9c9a5a0ff04",
   "metadata": {},
   "source": [
    "# South African COVID-19 Daily Hospital data API\n",
    "### Data engineering project continued\n",
    "#### Transformation phase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046ffc6-1fe5-4671-9f33-0e54d4e04132",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70f032-ed2e-45fd-8378-b45b5b7ba939",
   "metadata": {},
   "source": [
    "The API and documentation is available on <a href='https://covidza-data.deta.dev/docs'>COVIDZA DATA API</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c355de16-bc66-45c9-977e-e4fa7cc83358",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b7c0f-15c0-4e43-8732-afa907fd3eab",
   "metadata": {},
   "source": [
    "Looking for a data engineer, contact me:\n",
    " - [Linkedin]('https://linkedin.com/in/mpho-jan-kubeka')\n",
    " - [Github]('https://github.com/dataprojectswithMJ')\n",
    " - [Youtube]('https://www.youtube.com/channel/UClOP0fAisga04q3OB1iC4nQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83362968-385f-4efe-a6ce-028093fefb28",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96716bc2-15a6-4d1e-b2a8-aa9a31ab7527",
   "metadata": {},
   "source": [
    "## Problem with Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbfcdd5-f108-4acc-af90-5119bf0bbc7f",
   "metadata": {},
   "source": [
    "The biggest issues with this stage are:\n",
    "   - Extracting the data from the PDFs in the correct formats\n",
    "   - Setting up the correct data types\n",
    "\n",
    "<img src='PDF%20Data.png' width='500'>\n",
    "\n",
    "This screenshot shows the table on the PDF that contains the data that I will be using. Now I have to figure out formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b07f3-fe5a-435c-8e5e-8ba46721b911",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Solution:\n",
    "#### How did I get the data from the PDF documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d685cc-33d3-4e12-b2fb-4142688d35b5",
   "metadata": {},
   "source": [
    "I have 2 ways:\n",
    "  1) __Programmatically__\n",
    "      - This method means writing code that will read the PDF, annotate the table and then transform the data into the needed format.\n",
    "      \n",
    "      - There are 2 Python packages that I can use to annotate/read the table from the PDF:\n",
    "        1) Camelot\n",
    "           - Not the best.\n",
    "        2) Tabula\n",
    "           - Works fairly well but has issues remaining stable because of formatting styles in the PDFs.\n",
    "           \n",
    "  2) __Manually__\n",
    "      - This method is much more accurate but very inefficient as each annotation is done by hand.\n",
    "      - Remember __tabula__?\n",
    "            There is a web browser tool that gives you a GUI to click and drag a shape over the data you need. From there onwards you can download the image in a CSV format for my use case(more formats available such as JSON, TSV etc).\n",
    "            \n",
    "<img src='tabula_annotation.png' width='500'> <img src='tabula_export.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d5e4f-eada-4755-b66a-76cd40b1714e",
   "metadata": {},
   "source": [
    "Because of the instability of the styles of formatting, I chose to do this _**manually**_. Though inefficient, it is extremely accurate.\n",
    "\n",
    "I then export the annotated CSVs to a folder that I can iterate through."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef71c9b-398f-47e7-8cd5-d50ca88cd8a6",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed8175-7189-4454-9b0a-cb128160df49",
   "metadata": {},
   "source": [
    "My tech stack for this stage includes:\n",
    "   - Python\n",
    "        - Pandas\n",
    "        - PyMONGO (MongoDB Python Wrapper)\n",
    "        <br></br>\n",
    "   - MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529039d1-d1a0-473f-b8e3-b61ea979f197",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da34631c-02f5-4ef5-85d9-3109dc668bb0",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba418d5c-b29d-4653-b791-4ae127815523",
   "metadata": {},
   "source": [
    "### Importing the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95147add-def0-441f-b9f3-1fb69ded7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "from bson.json_util import dumps,loads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3af4a-4a2e-485f-8e8b-586162bec21e",
   "metadata": {},
   "source": [
    "I use __glob__ to find all the annotated CSVs and iterate over them. I use __Pandas__ instead of something like PySpark because there is little data to deal with rather than millions of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d31d59a-8b08-484e-8d1f-011ebcab3698",
   "metadata": {},
   "source": [
    "### Configure MongoDB instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed5af7b-19eb-462e-99ba-2edb45353f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = ' .... ' #uri is used to connect this wrapper to the database. Localhost for testing. Use ATLAS for cloud instance of MongoDB\n",
    "\n",
    "client = MongoClient(uri)\n",
    "\n",
    "db = client['COVIDAPI'] # Name of database where all the data will go into\n",
    "\n",
    "links = db['records'] #Name of the collection (not table because this is a NoSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff472d-c972-40cd-a0fa-e7eb4840a1aa",
   "metadata": {},
   "source": [
    "I use the same configurations as the extraction stage except for the collection name. This makes it so that the data can be found under 1 database but in different collections (tables in SQL terminology)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932071b3-e00d-41c1-ad9e-fc122773f89b",
   "metadata": {},
   "source": [
    "### Find the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b1b530f-782d-4298-b325-5dec8d65fa6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tabula_annotation.png'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = glob.glob('tabula*') #Use a term that is common to all file names. Could even use '*.csv'. Which will extract all CSVs\n",
    "filename[0] #Just to check if it was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e968da93-4d09-4bbd-ac20-376711f4a27d",
   "metadata": {},
   "source": [
    "### Now I can start reading the CSVs and doing some data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c421d-d25c-43ea-8f55-0b5c257374a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in filename:\n",
    "    \n",
    "    #Part 1:\n",
    "    df = pd.read_csv(files)\n",
    "    df1 = pd.DataFrame()\n",
    "\n",
    "    print('Dataframe set up successful')\n",
    "    \n",
    "    #Part 2:\n",
    "    intervals = [0, 3, 6, 9, 12, 15, 18, 21, 24]\n",
    "\n",
    "    for x in intervals:\n",
    "        df1 = df1.append(df.loc[x])\n",
    "        \n",
    "    df1.reset_index(drop='index', inplace=True)\n",
    "    \n",
    "    #Part 3:\n",
    "    try:\n",
    "        date = datetime.strptime(files.replace(\"tabula-\", \"\").replace(\".csv\", \"\").replace(\"Tabula CSV's\\\\\", \"\"),\n",
    "                                 '%d %b %Y')\n",
    "    except:\n",
    "        date = datetime.strptime(files.replace(\"tabula-\", \"\").replace(\".csv\", \"\").replace(\"Tabula CSV's\\\\\", \"\"),\n",
    "                                 '%d %B %Y')\n",
    "    new_titles = []\n",
    "\n",
    "    print(f'Working on {date} data')\n",
    "    \n",
    "    #Part 4:\n",
    "    if df1.columns[2] == 'Admissions to Date':\n",
    "        titles = ['Unnamed: 0', 'Facilities\\rReporting', 'Admissions to Date',\n",
    "                  'Died to\\rDate', 'Discharged\\rto date', 'Currently\\rAdmitted',\n",
    "                  'Currently\\rin ICU', 'Currently\\rVentilated', 'Currently\\rOxygenated',\n",
    "                  'Admissions in\\rPrevious Day']\n",
    "    else:\n",
    "        titles = ['Unnamed: 0', 'Facilities\\rReporting', 'Admissions\\rto Date',\n",
    "                  'Died to\\rDate', 'Discharged\\rto date', 'Currently\\rAdmitted',\n",
    "                  'Currently\\rin ICU', 'Currently\\rVentilated', 'Currently\\rOxygenated',\n",
    "                  'Admissions in\\rPrevious Day']\n",
    "\n",
    "    for x in titles:\n",
    "        new_titles.append(x.replace(\"\\r\", \"\").replace(\" \", \"\"))\n",
    "\n",
    "    if df1.columns[0] == 'Unnamed: 0':\n",
    "        new_titles[0] = 'Province'\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #Part 5:\n",
    "    column_names = dict(zip(titles, new_titles))\n",
    "\n",
    "    df1 = df1.rename(columns=column_names)\n",
    "    print('Column name changes successful')\n",
    "\n",
    "\n",
    "    for column in df1.columns:\n",
    "        if column == 'AdmissionsinPreviousDay':\n",
    "            df1.drop('AdmissionsinPreviousDay', axis=1, inplace=True)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #Part 6:\n",
    "    df1 = df1.append({'Province': 'National',\n",
    "                      'FacilitiesReporting': df1['FacilitiesReporting'].sum(),\n",
    "                      'AdmissionstoDate': df1['AdmissionstoDate'].sum(),\n",
    "                      'DiedtoDate': df1['DiedtoDate'].sum(),\n",
    "                      'Dischargedtodate': df1['Dischargedtodate'].sum(),\n",
    "                      'CurrentlyAdmitted': df1['CurrentlyAdmitted'].sum(),\n",
    "                      'CurrentlyinICU': df1['CurrentlyinICU'].sum(),\n",
    "                      'CurrentlyVentilated': df1['CurrentlyVentilated'].sum(),\n",
    "                      'CurrentlyOxygenated': df1['CurrentlyOxygenated'].sum()}, ignore_index=True)\n",
    "\n",
    "\n",
    "    df1['Date'] = date\n",
    "    \n",
    "    print('Adding totals to dataframe successful..')\n",
    "    \n",
    "    #Part 7:    \n",
    "    data = df1.to_dict(orient='records')\n",
    "    print('Data conversion complete.')\n",
    "    \n",
    "    #Part 8:\n",
    "    records.insert_many(data)\n",
    "\n",
    "    #Part 9:\n",
    "    move_file = files.replace(\"Tabula CSVs\\\\\", \"\")\n",
    "    \n",
    "    initial_file_path = ' ..... '\n",
    "    move_file_path\n",
    "\n",
    "    os.replace(f\"{initial_file_path}{move_file}\",\n",
    "               f\"{move_file_path}{move_file}\")\n",
    "\n",
    "    print('File moved Successfully!')\n",
    "\n",
    "    print(f'Upload to DB for {date} Successful')\n",
    "    print('----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145214fb-67dc-4ddf-9d5e-d3a72c7ea856",
   "metadata": {},
   "source": [
    "There is a lot going on in the above cell (98 lines of code in total). I seperated the code into parts that reader can use to understand the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28867fe-f340-4e76-82d4-fa6077abb988",
   "metadata": {},
   "source": [
    "## PART: \n",
    "  1) I create 2 dataframes (1 for entering the data and another for storing the cleaned data.\n",
    "  <br></br>\n",
    "  2) The most important values are the Provinces. This stage uses their index from the old dataframe to the new one.\n",
    "    <br></br>\n",
    "  3) This stage I use the try and except method to configure the acceptable datetime format. Remeber the __filename__ varible from glob, it stores the names of the files and in the extraction stage I stored the date as the file name. Now that action comes into play. I also replaced extra text (ie tabula, .csv etc) to only keep the __date__ from the name.\n",
    "    <br></br>\n",
    "  4) I stored the column names in a list called __titles__ and upon analysing the it, I noticed that sometimes the column 'Admissions to Date' is formatted differently. Fortunately I discovered only 2 ways this is done either with whitespaces or sometimes with __\"\\r\"__ in between the text, so it is fairly easy to deal with. That list is iterated through and then the column names are cleaned free of whitespaces and characters stored in a new list called __new_titles__. Sometimes the first column is stored as __'Unnamed: 0'__ so I switch it to __'Province'__.\n",
    "    <br></br>\n",
    " 5) Now I zip together the 2 lists (titles and new_titles) together in order to rename the dataframe columns to clean column names. There is a __'AdmissionsinPreviousDay'__ column which is not necessary for this project so I remove it.\n",
    "    <br></br>\n",
    " 6) The only thing missing now is the __Total__ count for the record. Using a dataframe append function, I am able to sum up each column and add the sum to the last row which named __'National'__ because the sum is all provinces combined. I also add the date to each row. This is so that when each record is loaded to the database it will have a date included and can be filtered by it.\n",
    "     <br></br>\n",
    " 7) Now I convert the dataframe to a dictionary using the 'to_dict' function with records orientation\n",
    "     <br></br>\n",
    " ### Part 8 is the *LOADING* step in this ETL process\n",
    " 8) I use __'insert_many'__ because the __'to_dict'__ function stores the each row as a dictionary. So the dictionary is a list of dictionaries. The reason why I need them in a dictionary is because PyMongo uses dictionaries to insert new data.\n",
    "     <br></br>\n",
    " 9) This is where I conclude the script. I have 2 variables __initial_file_path__ and __move_file_path__. This is for moving the files that the code is done with to another location. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
